{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importações das dependencias \n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import  *\n",
    "from pyspark.sql.types import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uma SparkSession pode ser usada para criar um DataFrame, registrar um DataFramecomo tabelas, \n",
    "#executar SQL em tabelas, armazenar tabelas em cache e ler arquivos em parquet.\n",
    "'''\n",
    "\n",
    "@builder = a classe SparkSession deve porssuir um atributo construtor.\n",
    "@master()= recebe parametros do timpo URL, onde se defime a url principal para o spark se conectar\n",
    "@appName() = seta um nome para ser exebido no espark UI, caso esta estancia não seja nomeada por padrão será gerado um valor randomico.\n",
    "@config() = define as opçõe do arquivo de configurações, caso não definido é adicionado um metodo padrão\n",
    "\n",
    "'''\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Spark Learning\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realiza a leitura para o data frame\n",
    "\n",
    "'''\n",
    "O spark suporta uma grande variedade de operações com inumeras fontes de dados,\n",
    "que podem ser acessados a partir da insterface de um data frame. Um data frame é uma\n",
    "abristração de dados temporaria onde pode-se realizar  consultas sql e traformações oriundas de viwes temporarias.\n",
    "Exemplo de algumas estruturuas de dados que podem ser carregadas para o data frame:\n",
    "(json, parquet, jdbc, orc, libsvm, csv, text)\n",
    "\n",
    "!Json para leitura de estruturas de dados tipo json, se utiliza o padrão json lines disponivel em http://jsonlines.org/  \n",
    "JSON Lines é um formato conveniente para armazenar dados estruturados que podem ser processados um registro por vez. \n",
    "\n",
    "\n",
    "'''\n",
    "dfJson = spark.read.json('/data/data.json') \n",
    "\n",
    "\n",
    "textFile = spark.read.text(\"/data/requirements.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|          value|\n",
      "+---------------+\n",
      "|     geopandas |\n",
      "|         geopy |\n",
      "|         rtree |\n",
      "|         folium|\n",
      "|         numpy |\n",
      "|        shapely|\n",
      "|         fiona |\n",
      "|           six |\n",
      "|        pyproj |\n",
      "|numexpr==2.6.4 |\n",
      "| elasticsearch |\n",
      "|       geojson |\n",
      "|        plotly |\n",
      "|          tqdm |\n",
      "|      mapboxgl |\n",
      "|     cufflinks |\n",
      "|      geohash2 |\n",
      "|        tables |\n",
      "|      mixpanel |\n",
      "|    SQLAlchemy |\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "| six |\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numero de linhas de um data farme\n",
    "textFile.count()\n",
    "\n",
    "#retorna a primeira linha do data frame\n",
    "textFile.first()\n",
    "\n",
    "#mostra o data frame\n",
    "textFile.show()\n",
    "\n",
    "#condição parece com where do SQL\n",
    "textFile.filter(\"value == 'six '\").show()\n",
    "\n",
    "#tambem um filtro condicional que pervorre todo dataframe\n",
    "textFile.filter(textFile.value.contains('geopy')).count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|          value|\n",
      "+---------------+\n",
      "|     geopandas |\n",
      "|         geopy |\n",
      "|         rtree |\n",
      "|         folium|\n",
      "|         numpy |\n",
      "|        shapely|\n",
      "|         fiona |\n",
      "|           six |\n",
      "|        pyproj |\n",
      "|numexpr==2.6.4 |\n",
      "| elasticsearch |\n",
      "|       geojson |\n",
      "|        plotly |\n",
      "|          tqdm |\n",
      "|      mapboxgl |\n",
      "|     cufflinks |\n",
      "|      geohash2 |\n",
      "|        tables |\n",
      "|      mixpanel |\n",
      "|    SQLAlchemy |\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# o cache serve para mante os dados em memoria para otimização de cálculos parciais. que podem ser utilizados em porcessamentos subsegquentes.\n",
    "# o cache utiliza aoenas um nivel de amarzwnamento em memoria \n",
    "textFile.cache().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
